{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9322536,"sourceType":"datasetVersion","datasetId":5647272}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data Collection and processing**","metadata":{}},{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nprint('started')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndf_combined = pd.read_csv('/kaggle/input/preprocessed-post-csv/preprocessed_post.csv')\n\ndf_combined.dropna(subset=['processed_post'], inplace=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-08T08:36:14.060164Z","iopub.execute_input":"2024-09-08T08:36:14.060662Z","iopub.status.idle":"2024-09-08T08:36:15.516435Z","shell.execute_reply.started":"2024-09-08T08:36:14.060616Z","shell.execute_reply":"2024-09-08T08:36:15.515311Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"started\n/kaggle/input/preprocessed-post-csv/preprocessed_post.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Embedding**","metadata":{}},{"cell_type":"code","source":"import gensim\n\nsentences = [post.split() for post in df_combined['processed_post']]\nprint('sentences',len(sentences))\nword2vec_model = gensim.models.Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n\nvocab_size = len(word2vec_model.wv)\nprint('123456',vocab_size)\n\n# def embed_sentence_average(sentence, word2vec_model):\n#     embeddings = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n   \n#     if len(embeddings) == 0:\n#         return np.zeros(300)  # Return a zero vector if no embeddings are found\n#     return np.mean(embeddings, axis=0)  \n#     #return np.array([word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(300) for word in sentence])\n\n# # import numpy as np\n# # embeded_post= [embed_sentence(post.split(), word2vec_model) for post in df_combined['processed_post']]\n\n# X_features = np.array([embed_sentence_average(post.split(), word2vec_model) for post in df_combined['processed_post']])\n# print('X featured', X_features[:2], len(X_features))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:36:40.375915Z","iopub.execute_input":"2024-09-08T08:36:40.376339Z","iopub.status.idle":"2024-09-08T08:37:28.357172Z","shell.execute_reply.started":"2024-09-08T08:36:40.376299Z","shell.execute_reply":"2024-09-08T08:37:28.355814Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"sentences 101357\n123456 100486\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Padding**","metadata":{}},{"cell_type":"code","source":"def embed_sentence(sentence, word2vec_model):\n    return np.array([word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(300) for word in sentence])\n\nimport numpy as np\nembeded_post= [embed_sentence(post.split(), word2vec_model) for post in df_combined['processed_post']]\nfrom keras.preprocessing.sequence import pad_sequences\n#X_padded = pad_sequences([x for x in embeded_post], maxlen=300, dtype='float32', padding='post')\nmaxlen = 100\n\n# Pad sequences with maxlen\nX_padded = pad_sequences([x for x in embeded_post], maxlen=maxlen, dtype='float32', padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:41:50.414161Z","iopub.execute_input":"2024-09-08T08:41:50.415353Z","iopub.status.idle":"2024-09-08T08:42:08.738022Z","shell.execute_reply.started":"2024-09-08T08:41:50.415286Z","shell.execute_reply":"2024-09-08T08:42:08.736643Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_features.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T06:32:59.358190Z","iopub.execute_input":"2024-09-08T06:32:59.358697Z","iopub.status.idle":"2024-09-08T06:32:59.368497Z","shell.execute_reply.started":"2024-09-08T06:32:59.358651Z","shell.execute_reply":"2024-09-08T06:32:59.367000Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(101357, 300)"},"metadata":{}}]},{"cell_type":"code","source":"df_combined['label'].shape","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:36:00.796455Z","iopub.execute_input":"2024-09-08T08:36:00.796957Z","iopub.status.idle":"2024-09-08T08:36:00.840940Z","shell.execute_reply.started":"2024-09-08T08:36:00.796910Z","shell.execute_reply":"2024-09-08T08:36:00.839329Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_combined\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mNameError\u001b[0m: name 'df_combined' is not defined"],"ename":"NameError","evalue":"name 'df_combined' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_features, df_combined['label'], test_size=0.2, random_state=42, stratify=df_combined['label'])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T06:33:31.026740Z","iopub.execute_input":"2024-09-08T06:33:31.027401Z","iopub.status.idle":"2024-09-08T06:33:31.410741Z","shell.execute_reply.started":"2024-09-08T06:33:31.027341Z","shell.execute_reply":"2024-09-08T06:33:31.409322Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y_train = np.array(y_train)\ny_test = np.array(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T04:57:43.536949Z","iopub.execute_input":"2024-09-08T04:57:43.537451Z","iopub.status.idle":"2024-09-08T04:57:43.544600Z","shell.execute_reply.started":"2024-09-08T04:57:43.537405Z","shell.execute_reply":"2024-09-08T04:57:43.543123Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, X_test.shape,'test train split')\nprint(np.bincount(y_train),np.bincount(y_test), ' ration of 0:1')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T06:33:48.123915Z","iopub.execute_input":"2024-09-08T06:33:48.124490Z","iopub.status.idle":"2024-09-08T06:33:48.134013Z","shell.execute_reply.started":"2024-09-08T06:33:48.124432Z","shell.execute_reply":"2024-09-08T06:33:48.132708Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(81085, 300) (20272, 300) test train split\n[70601 10484] [17651  2621]  ration of 0:1\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Create the RandomForestClassifier\nprint(\"model started\")\nrf_clf = RandomForestClassifier(n_estimators=200, random_state=42)\nprint(\"fitting started\")\n# Fit the model on your training data\nrf_clf.fit(X_train, y_train)\nprint(\"prediction started\")\n# Predict on test data\ny_pred_rf = rf_clf.predict(X_test)\n\n# Print classification report\nprint(\"Random Forest Classifier:\")\nprint(classification_report(y_test, y_pred_rf))","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:07:50.515050Z","iopub.execute_input":"2024-09-08T07:07:50.515755Z","iopub.status.idle":"2024-09-08T07:19:02.398457Z","shell.execute_reply.started":"2024-09-08T07:07:50.515705Z","shell.execute_reply":"2024-09-08T07:19:02.397022Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"model started\nfitting started\nprediction started\nRandom Forest Classifier:\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97     17651\n           1       0.86      0.69      0.77      2621\n\n    accuracy                           0.95     20272\n   macro avg       0.91      0.84      0.87     20272\nweighted avg       0.94      0.95      0.94     20272\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n# Create the MultinomialNB model with alpha parameter (smoothing)\nnb_clf = MultinomialNB(alpha=1.0)\n\n# Fit the model on your training data\nnb_clf.fit(X_train, y_train)\n\n# Predict on test data\ny_pred_nb = nb_clf.predict(X_test)\n\n# Print classification report\nprint(\"Multinomial Naive Bayes Classifier:\")\nprint(classification_report(y_test, y_pred_nb))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T06:50:33.863886Z","iopub.execute_input":"2024-09-08T06:50:33.865201Z","iopub.status.idle":"2024-09-08T06:50:34.287907Z","shell.execute_reply.started":"2024-09-08T06:50:33.865133Z","shell.execute_reply":"2024-09-08T06:50:34.285426Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m nb_clf \u001b[38;5;241m=\u001b[39m MultinomialNB(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the model on your training data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mnb_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m y_pred_nb \u001b[38;5;241m=\u001b[39m nb_clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:776\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    774\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:898\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 898\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1418\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1415\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n","\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"],"ename":"ValueError","evalue":"Negative values in data passed to MultinomialNB (input X)","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Use TfidfVectorizer to convert text data to TF-IDF representation\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(df_combined['processed_post'])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:06:19.365546Z","iopub.execute_input":"2024-09-08T07:06:19.366035Z","iopub.status.idle":"2024-09-08T07:06:23.772143Z","shell.execute_reply.started":"2024-09-08T07:06:19.365982Z","shell.execute_reply":"2024-09-08T07:06:23.770725Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X_tfidf, df_combined['label'], test_size=0.2, random_state=42, stratify=df_combined['label'])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:06:33.576923Z","iopub.execute_input":"2024-09-08T07:06:33.577456Z","iopub.status.idle":"2024-09-08T07:06:33.653794Z","shell.execute_reply.started":"2024-09-08T07:06:33.577406Z","shell.execute_reply":"2024-09-08T07:06:33.652594Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(X_train_nb.shape, X_test_nb.shape,'test train split')\nprint(np.bincount(y_train_nb),np.bincount(y_test_nb), ' ration of 0:1')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T09:05:26.264109Z","iopub.execute_input":"2024-09-08T09:05:26.265249Z","iopub.status.idle":"2024-09-08T09:05:26.720929Z","shell.execute_reply.started":"2024-09-08T09:05:26.265188Z","shell.execute_reply":"2024-09-08T09:05:26.719239Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train_nb\u001b[49m\u001b[38;5;241m.\u001b[39mshape, X_test_nb\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest train split\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mbincount(y_train_nb),np\u001b[38;5;241m.\u001b[39mbincount(y_test_nb), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ration of 0:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'X_train_nb' is not defined"],"ename":"NameError","evalue":"name 'X_train_nb' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Train a MultinomialNB model\nnb_clf = MultinomialNB(alpha=1.0)\nnb_clf.fit(X_train_nb, y_train_nb)\ny_pred_nb = nb_clf.predict(X_test_nb)\n\n# Print classification report\nprint(\"Multinomial Naive Bayes Classifier:\")\nprint(classification_report(y_test_nb, y_pred_nb))","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:07:23.646191Z","iopub.execute_input":"2024-09-08T07:07:23.646885Z","iopub.status.idle":"2024-09-08T07:07:23.740893Z","shell.execute_reply.started":"2024-09-08T07:07:23.646815Z","shell.execute_reply":"2024-09-08T07:07:23.739395Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Multinomial Naive Bayes Classifier:\n              precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96     17651\n           1       0.91      0.48      0.63      2621\n\n    accuracy                           0.93     20272\n   macro avg       0.92      0.73      0.79     20272\nweighted avg       0.92      0.93      0.92     20272\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_padded, df_combined['label'], test_size=0.2, random_state=42, stratify=df_combined['label'])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:42:54.792100Z","iopub.execute_input":"2024-09-08T08:42:54.792608Z","iopub.status.idle":"2024-09-08T08:42:58.623750Z","shell.execute_reply.started":"2024-09-08T08:42:54.792561Z","shell.execute_reply":"2024-09-08T08:42:58.622355Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"y_train_nn = np.array(y_train_nn)\ny_test_nn = np.array(y_test_nn)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:43:42.138675Z","iopub.execute_input":"2024-09-08T08:43:42.139174Z","iopub.status.idle":"2024-09-08T08:43:42.145642Z","shell.execute_reply.started":"2024-09-08T08:43:42.139128Z","shell.execute_reply":"2024-09-08T08:43:42.144391Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(X_train_nn.shape, X_test_nn.shape,'test train split')\nprint(np.bincount(y_train_nn),np.bincount(y_test_nn), ' ration of 0:1')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:44:21.544482Z","iopub.execute_input":"2024-09-08T08:44:21.544997Z","iopub.status.idle":"2024-09-08T08:44:21.553342Z","shell.execute_reply.started":"2024-09-08T08:44:21.544937Z","shell.execute_reply":"2024-09-08T08:44:21.551717Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(81085, 100, 300) (20272, 100, 300) test train split\n[70601 10484] [17651  2621]  ration of 0:1\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.utils import class_weight\n\n# Assuming y_train contains your labels (0 for non-suicidal, 1 for suicidal)\nclass_weights = class_weight.compute_class_weight('balanced', classes=[0, 1], y=y_train_nn)\n\n# Create a dictionary for class weights\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n\nprint(class_weight_dict)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:45:06.946877Z","iopub.execute_input":"2024-09-08T08:45:06.948140Z","iopub.status.idle":"2024-09-08T08:45:06.982137Z","shell.execute_reply.started":"2024-09-08T08:45:06.948084Z","shell.execute_reply":"2024-09-08T08:45:06.980744Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{0: 0.5742482401099135, 1: 3.867083174360931}\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import GRU, Dense, Dropout, Input\n\n# Assuming you have defined y_train (your target labels)\nx2_model = Sequential([\n        Input(shape=(X_train_nn.shape[1], X_train_nn.shape[2])),\n    GRU(128, return_sequences=True),\n    Dropout(0.2),\n    GRU(64),\n    Dropout(0.2),\n    Dense(1, activation='sigmoid')\n])\n\nx2_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nx2_model.fit(X_train_nn, y_train_nn, epochs=10, batch_size=32, class_weight=class_weight_dict)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:46:05.264438Z","iopub.execute_input":"2024-09-08T08:46:05.264927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}